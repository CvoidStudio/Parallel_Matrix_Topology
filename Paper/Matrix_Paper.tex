%------------------------------------------------------------------------------
% BEGINING
%------------------------------------------------------------------------------
\documentclass{amsart}
% Packages
\usepackage[pdftex]{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{changepage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage[titletoc]{appendix}
%\usepackage[
%	backend=biber,
%	style=ieee,
%]{biblatex}
\usepackage{csquotes}
\usepackage[colorlinks,linkcolor=blue]{hyperref}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{GENERAL CONSTRUCTION AND ANALYSIS OF \\ PARALLEL MATRIX MULTIPLICATION ALGORITHMS}

%    Information for first author
\author{WENJING CUN}
%    Address of record for the research reported here
\address{Department of Applied Mathematics, Stonybrook Univ. SUNY, 
Stony Brook, New York 11790}
%    Current address
%\curraddr{Department of Mathematics and Statistics,
%Case Western Reserve University, Cleveland, Ohio 43403}
\email{wenjing.cun@stonybrook.edu}
%    \thanks will become a 1st page footnote.
%\thanks{The first author was supported in part by NSF Grant \#000000.}

%    Information for second author
\author{LIHUA PEI}
\address{Department of Applied Mathematics, Stonybrook Univ. SUNY, 
Stony Brook, New York 11790}
\email{lihua.pei@stonybrook.edu}
%\thanks{Support information for the second author.}

%    Information for second author
\author{YUEFAN DENG}
\address{Department of Applied Mathematics, Stonybrook Univ. SUNY, 
Stony Brook, New York 11790}
\email{yuefan.deng@stonybrook.edu}
%\thanks{Support information for the coord author.}

%    General info
%\subjclass[2000]{Primary 54C40, 14E20; Secondary 46E25, 20C20}

\date{March 29, 2018}

\keywords{Parallel Computation, Matrix Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
	Algorithms of parallel matrix multiplication for distributed-memory systems are analyzed and a map of the universal parallel matrix multiplication algorithms, based on 3-dimensional hypercube with computational complexity of $O(N^3)$ are presembled. Numerical experiments of several algorithms on clusters were carried out to compare the performace. 
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
	High-performance computing (HPC) algorithms for linear algebra are critical for many application in science, engineering and commerce. Matrix multiplication (MM) is the heart of many linear algebra problems. Much efforts have been devoted to discuss the general method for MM  and the parallelization of it. Several classical methods have attracted wide acceptance. It is desirable to design a map for organizing the family of parallel matrix multiplication (PMM) algorithms. With the metric of parallel efficiency of the lattice, it is now possible to evaluate the upper bound of the algorithms depending on any given dimensions of the matrices.\par

	The rest of the manuscript is organized as follows: Section 2 presents the general idea of MM  on 3D hypercube, based the maps between sets of coordinates. Section 3  constructs the communication forest among the computation nodes, while proceeding the parallel algorithms, and organizing the family of the parallel algorithms based on 3D hypercube. Section 4 analyzes several classical PMM algorithms and propose a general algorithm for universal MM. Finally, Section 5 discusses the performance and the reduction of the computational complexity, and summarizes the construction of algorithms.\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~\\
\section{Construction of matrix multiplication in 3D hypercube}
Firstly, we shall review the naïve algorithm of MM.\par

%% --Algorithm 2.1--%%
\begin{algorithm}[t] 
\caption{(Naive Algorithm)}
\begin{adjustwidth}{0cm}{1cm} 
\textbf{Input:} $A\in \mathbb{R}^{m\times l},B\in \mathbb{R}^{l\times n}$\\
\textbf{Output:} $C\in \mathbb{R}^{m\times n}$\\
\textbf{for} $i=1:m $\textbf{ do}\\
\end{adjustwidth}
	\begin{adjustwidth}{1cm}{1cm}
	\textbf{for} $i=1:l $\textbf{ do}\\
	\end{adjustwidth}
		\begin{adjustwidth}{2cm}{1cm}
		\textbf{for} $i=1:n $\textbf{ do}\\
		\end{adjustwidth}
			\begin{adjustwidth}{3cm}{1cm}
			$C_{ik}=C_{ik}+A_{ij}\cdot B_{jk}$\\
			\end{adjustwidth}
\begin{adjustwidth}{0cm}{1cm} 
\textbf{Return } $C$\\
\end{adjustwidth}
\end{algorithm}
%% --END Alg 2.1-- %%

Naïve algorithm is the direct definition of MM s for any given pair of matrices: $A\in R^{(m\times l)},B\in R^{(l\times n)}$, where $m,l$ and n are positive integers. Focusing on one final product $C=A\cdot B$, namely $C_{ij}$, we have:
\begin{equation}
C_{ik}=\sum_{j=1}^{l}(A_{ij}\cdot B_{jk})
\end{equation}

which dependes on the matrix elements at three indices $i$,$j$ and $k$, namely $(i,j,k)$. Thus, grouping all $(i,j,k)$ in $A
$ and $B$ may produce a well-structured map from the set $(i,j,k)$ to the set $(i,k)$ for $C$ and this map is expected to be independent of the actual values of the matrix element.


%Then regardless of the actual values of $A_{ij}$ and $B_{jk}$, formula (1) reveals the information that $C_{ik}$ only results from the values in the coordinates depending on three indices i,j and k, namely $(i,j,k)$. Then group all $(i,j,k)$ corresponding to the coordinates in $A$ and $B$ together, we may expect there is a well-defined map from such a set of $(i,j,k)$ to that of $(i,k)$ in $C$.\par
%Sourced from such an idea, and in order to make the algorithms totally independent of the values involved, so that can be flexible and robust, we will define such a map from a set of coordinates to another one in the first subsection.\par


%% ------------ Subsection 2.1 ------------ %%
\subsection{Coordinate Set and Operation Map}
Maps between sets of values with associate coordinates are often independent of the values involved, but more like the relationship among the indices of coordinates. So to abstract such relationships, we define the following \textbf{operation map}:

\begin{definition}
An \textbf{operation map} is a map from one set A to another set B, where there are two injective maps $L_1:C\to A, L_2:C\to B$, and C is a set with a well-defined operation function $\tau$.
\end{definition}

For convenience, we denote such an operation map $f$ as: $f_{C,\tau}:A\to B$, and name $L_1,L_2$ as the \emph{\textbf{coordinate maps}}. Here $A$ and $B$ are the topologies from which the sets of values in $C$ abstracted into, and sourcing from the most intuitive concept in geometry, we call $A$ and $B$ the corresponding \emph{\textbf{coordinate maps}}. Among the many properties, we list the first one for the operation map:
% Theorem 2.1
\begin{theorem}
Given an operation map $f_{C,\tau}:C_1\to C_2$, if $S$ is closed under a well-defined operation $\tau$ , or say $G$ is an algebraic group, then for any subset $S\subseteq G$, $f_{C,\tau}:C_1\to C_2$ is also an operation map. Vice Versa.
\end{theorem}
This is the stability property of an operation map, i.e., the operation map is stable regardless of the values when the inputs in conservative of the algebraic operation.\par

For a series of operation maps, \textbf{chain rule} applies:
% Theorem 2.2
\begin{theorem}
Given a set $G$ that is closed under a series of operations $\tau_1,...,\tau_n$, a series of sets $C_1,...,C_n$, also a series of subsets of $G, S_1,...,S_n$, there exist at least $n$ operation maps:

%\begin{equation}
%(f_1)_{S_1,\tau_1},...,(f_n)_{S_n,\tau_n}
%\end{equation}

where $(f_1)_{S_1,\tau_1},...,(f_n)_{S_n,\tau_n}$ is also an operation map.
\end{theorem}

Following the chain rule in the opposite direction, if an abstract operation map $f_(G,\tau):C_1\to C_2$ can be decomposed into a series of operation maps, then $f_(G,\tau)$ is \emph{\textbf{separable}}.\par

The third property shows the flexibility, or the scalability in computational engineering, if the operation map can be conservative in topology:
% Theorem 2.3
\begin{theorem}
Given two homomorphic algebraic structures $G_1$ and $G_2$, closed under two operations $\tau_1$ and $\tau_2$ respectively, if $S_1\subseteq G_1$ and $S_2\subseteq G_2$ are two subsets, and an operation map $f_(S_1,\tau_1):C_1\to C_2$, then $f_(S_2,\tau_2):C_1\to C_2$ is also an operation map.
\end{theorem}
We will next abstract the process of MM , based Algorithm 2.1, into a separable operation map composed by several independent stages. We also introduce the parallelization of the algorithm.\\



%% ------------ Subsection 2.2 ------------ %%
\subsection{Algorithms Based on 3-Dimensional Hypercube}
	To abstract Algorithm 2.1 into the operation maps into operations maps, we first split all the multiplications between a pair of scalars. Then, for the 3D-hypercube algorithm, we construct an operation map from a coordinate set $C_{11}$ to the other one $C_{12}$. Since the original coordinate set represents the positions of the elements of $A$ and $B$, whose coordinate are
\begin{equation}
C_A=\mathbb{Z}/{m\mathbb{Z}}\times \mathbb{Z}/{l\mathbb{Z}},
C_B=\mathbb{Z}/{l\mathbb{Z}}\times \mathbb{Z}/{n\mathbb{Z}}
\end{equation}

The operation map is
\begin{equation}
(f_1)_{\mathbb{R},\times}:C_A\times C_B\to C_{mult3D}
\end{equation}
where $C_{mult}=\mathbb{Z}/{m\mathbb{Z}}\times \mathbb{Z}/{l\mathbb{Z}}\times \mathbb{Z}/{n\mathbb{Z}}$, "$\times$" is the scalar multiplication and:
\begin{equation}
(f_1)_{\mathbb{R},\times}((r_a,c_a),(r_b,c_b))=
\left\{
\begin{array}{ll}  
             (r_a,c_b,c_a), &\text{if } c_a=r_b  \\ 
             \varnothing, &\text{otherwise}
\end{array}
\right.
\end{equation}
\par
	This operation map represents the process of $A_{ij}\cdot B_{jk}$, and from $C_{mult}$, it can be illustrated in Fig.1:
% figure 3D operation map #1
\begin{figure}[h]
	\includegraphics[scale=0.45]{Figures/Matrix3d.png}
	\caption{The operation map $(f_1)_(\mathbb{R},\times ):C_A\times C_B\to C_{\text{mult}}$}
	\label{Fig:1}
\end{figure}
~\\
~\\
\par
The next operation is that of addition. Since only the scalar operations remain, the result is a matrix  $C\in \mathbb{R}^{m\times n}$, thus we define the third coordinate set as
\begin{equation}
C_C=\mathbb{Z}/{m\mathbb{Z}}\times \mathbb{Z}/{n\mathbb{Z}}
\end{equation}\par
and the corresponding abstract operation map is 
\begin{equation}
\begin{array}{l}  
             (f_2)_{R,+}:C_{mult3D}\to C_C\\ 
             (f_2)_{R,+}((r_a,c_b,c_a))=(r_a,c_b)
\end{array}
\end{equation}
Here "+" is the scalar addition.

	%Observed from formula(2.7), all the 3D “cubes” in the Figure 2.1 with the same “z-value” are reduced in to one in the “xy-plane”, which can be plotted as the following:\par
	
	Equation $(2.7)$, reveals that all the 3D “cubes” in the Figure 1 with the same “z-value” are reduced in to one in the “xy-plane”, illustrated in Figure 2. \par
% figure 3D operation map #2
\begin{figure}[h]
	\includegraphics[scale=0.45]{Figures/Matrix3d_Reduce.png}
	\caption{Operation map $(f_2)_{F^{m\times l\times n},\text{sum}}$}
\end{figure}
~\\
\par

Now the whole process has been abstracted into a chain of abstract operation maps, for the MM  operation: $F^{m\times l}\times F^{l\times n}\to F^{m\times n}$, we can define the chain of abstract operation maps as the following:
\begin{equation}
f_{\mathbb{R}^{m\times l}\times\mathbb{R}^{l\times n},\text{matrix multiply}} = (f_1)_{\mathbb{R},\times}\circ(f_2)_{\mathbb{R},+}
\end{equation}
	where we define $f_{\mathbb{R}^{m\times l}\times\mathbb{R}^{l\times n},\text{matrix multiply}}(x)=x$, is a trivial abstract operation map directly representing that: $C=AB$.\par
	For a local-memory system, there seems nothing to modify (2.7) for such a fixed chain, but to a distributed-memory cluster, we can add one more abstract operation map to each computing node p, $g_{R,=}$, where "=" simply means the identity operation, but can filter the coordinates for each node, and we define it as:
\begin{equation}
\begin{array}{l}  
g_{\mathbb{R},=}:C_{mult3D}\to C_{mult3D}\\
g_{\mathbb{R},=}((r_a,c_b,c_a))=
	\left\{
	\begin{array}{ll}  
          (r_a,c_b,c_a), &\text{if } (r_a,c_b,c_a)\in U_p  \\ 
          \varnothing, &\text{otherwise}
	\end{array}
	\right.
\end{array}
\end{equation} 
\par
	Here $U_p$ is a custom set chosen for each node $p$, called a \emph{\textbf{filtering set}}, represents the “blocks” chosen for node $p$ to calculate. Thus, the chain of abstract operation maps for each node $p$ is written as:
\begin{equation}
(f_p)_{\mathbb{R}^{m\times l}\times\mathbb{R}^{l\times n},\text{matrix multiply}}=
(f_1)_{mathbb{R},\times}\circ g_{mathbb{R},=}\circ(f_2)_{mathbb{R},+}
\end{equation}
This formula represents all the value-independent parallel algorithms based on Naïve Algorithm.
\par
Now we take such a case for example: if there are totally 4 nodes for computing $A\cdot B$, where $A\in \mathbb{R}^{6\times4},B\in \mathbb{R}^{4\times6}$, then choose the four filtering sets as:
\begin{equation}
\begin{array}{l}  
      U_1=\{4,5\}\times\{2,3,4\}\times(\mathbb{Z}/4\mathbb{Z}) \\ U_2=\{3\}\times\{2,3,4\}\times(\mathbb{Z}/4\mathbb{Z})\\ 
      U_3=\{3,4,5\}\times\{0,1\}\times(\mathbb{Z}/4\mathbb{Z}) \\ U_4=\{0,1,2\}\times\{0,1,2,3,4\}\times(\mathbb{Z}/4\mathbb{Z}) 
\end{array}
\end{equation}
\par
So the “blocks” in ${C_mult}$ allotted to each node are shown as the following figure:\\
% figure 3D operation map #whole
\begin{figure}[h]
	\includegraphics[scale=0.35]{Figures/Matrix3d_Filter.png}
	\caption{Parallel 3D-Hypercube}
\end{figure}
\par
	In fact, the strategy to choose filtering sets as single “cubes” that all span over z-axis, is similar to one of the popular general algorithm for parallel universal MM, called BMR[1], if not consider the communication among the nodes. Specially, if the blocks are not finished computing at one communicating step, it can deduce many interesting algorithms, where Cannon’s algorithm[2] and SUMMA[3] are two of the famous ones.\par
	So only consider the computational part of the whole process of MM, all the algorithms based on Naive Algorithm can be deduced by selecting the filtering sets $\{U_p\}$, so that the algorithm is presented as the following pseudocode:\par

%% --Algorithm 2.2--%%
\begin{algorithm}[h] 
\caption{(3D-Hyperblock Matrix Multiplication)}
\begin{adjustwidth}{0cm}{1cm} 
\textbf{Input:} $A\in \mathbb{R}^{m\times l},B\in \mathbb{R}^{l\times n}$\\
\textbf{Output:} $C\in \mathbb{R}^{m\times n}$\\
\textbf{if} at rank $p$\textbf{ do}\\
\end{adjustwidth}
	\begin{adjustwidth}{1cm}{1cm}
	choose $U_p=\{(i_1,j_1,k_1),(i_2,j_2,k_2),...,(i_K,j_K,k_K)\}$\\
	\textbf{for} $i,j,k$ in $U_p$ \textbf{ do}
	\end{adjustwidth}
		\begin{adjustwidth}{2cm}{1cm}
		$C_{ik}=C_{ik}+A_{ij}\cdot B_{jk}$\\
		\end{adjustwidth}
\begin{adjustwidth}{0cm}{1cm} 
\textbf{Return } $C$\\
\end{adjustwidth}
\end{algorithm}
~\\~\\~\\
\par
	The members in the 3D-Hyperblock family often have the best potential in minimizing the time in communication, however, while dealing with large-scale matrices, the buffer size often becomes a problem for the cluster to limit the total size of data to be deployed onto each computation node. Therefore, each such optimization in communication involves deploying the best subset $V_p\subseteq U_p$, and find the best communication tree to pass the data among the nodes, in order to finish the tasks undertaken by $U_p$ efficiently, which is sophisticated while dealing with such $P$ cubes simultaneously.\par

	In the next section, we will discuss the influence of limiting buffer size on designing the communication part for a parallel algorithm, and find out a general map for designing the algorithm for PMM.\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Communication in 3d-hypercube}
	Allocation of blocks of operations to each computational node is straightforward for 3D hypercube algorithms, however, while dealing with large-scale MMs, the restricted buffer size will limit the size of data stored onside each node.\par
	Generally, when a parallel program allows all the nodes to own all the necessary data already, in terms of the algorithm itself, the cost of communication can be minimized. Otherwise, one node lacking necessary data needs to fetch those needed from the other ones. So for PMM, referring to algorithm 2.2, often we can only deploy part of $U_p$ onto node $p$, then following the communication tree, at each step, one need to finish as much as computation tasks with its data owing, release or send the data no longer needed, and require and receive the data needed for next step of computation.\par


%% ------------ Subsection 3.1 ------------ %%
\subsection{Communication Rules and Cost}
For convenience, we describe a “cube” assigned to node p with:\par
\begin{itemize}
\item $m_p,l_p,n_p,$ the lengths of the cube assigned to node $p$, along with x-axis, y-axis and z-axis respectively;
% figure parameters
\begin{figure}[h]
	\includegraphics[scale=0.45]{Figures/cube.jpg}
	\caption{Cube Onside Node $p$}
\end{figure}
\end{itemize}

	For many of the algorithms in 3D-hypercube family, we may assume that such $P$ communication trees are similar, or out of the geometric vision, such $P$ “cubes” are similar in the following properties:\par
\begin{itemize}
\item Volume, i.e. the number of multiplication between entries;
\item Total projection area onto xz-plane and yz-plane, i.e. the total size of data needed for the computation tasks;
\item Maximum area projective onto xz-plane and yz-plane can be covered at each communication step, i.e. the maximum buffer size assigned to each core, denoted as $\text{Buffer}_p$.
\end{itemize}

The three properties above in fact determined the performance of the part of scalar (or even block) multiplication. As for the part of scalar (or even block) addition, there is only one additional property needed to be considered:\par
\begin{itemize}
\item Total projection area onto xy-plane, i.e. the size of data to be reduced onto one node.
\end{itemize}

% Rules
Then to design a smart algorithm, we may design following the rules below:\\
\textbf{Rule 3.1} The total volume of the $P$ cubes is exactly $mln$, i.e. the total volume of the whole 3D-hypercube. i.e. there would be no repetitive computational tasks among the nodes:
\begin{equation}
\sum_{p=1}^{P}m_pl_pn_p=mln
\end{equation}
\\
\textbf{Rule 3.2} The entries of two matrices can be covered by the data stored in the $P$ nodes at each communication step, so that it requires:
\begin{equation}
\sum_{p=1}^{P}\text{Buffer}_p\ge ml+ln
\end{equation}
\\
\textbf{Rule 3.3} Any entry (or block) stored onside one node can only be released if and only if all the scalar (block) multiplications involving it have been finished. So that the total cost of receiving data on node $p$ is:
\begin{equation}
T_{p,recv}=C\cdot(m_pl_p+l_pn_p-\text{Buffer}_p)
\end{equation}

\par
	From Rule 3.3, since the total number of sending is always conservative with that of receiving, so that we may estimate the average cost of communication on send-receive for each node would follow: 
\begin{equation}
\left\{
	\begin{array}{ll}  
          T_{comm,send-recv}\le\sum_{p=1}^{P}T_{p,recv}  & \text{send/require one by one}\\ 
          T_{comm,send-recv}\ge\max \limits_{p}\{T_{p,recv}\} & \text{communication parallelized}
	\end{array}
\right.
\end{equation}
\\
where the lower bound comes from the strategy that minimizing the waiting time, where we let the one with the largest demand to send nothing out.\par
	Next, based on the analysis of cost on send-recv, we are going to discuss the strategy of designing the cubes and communication tree for each node.

%% ------------ Subsection 3.2 ------------ %%
\subsection{Algorithm for Parallel Matrix Multiplication}
	In practical cases, since where all the computation nodes are in the similar conditions (in CPU, GPU, RAM, etc.), we often divide the whole 3D hypercube into $P$ similar (not necessary to be identical) cubes, and assigned with similar limits of buffer sizes.\par
	For each cube assigned to each node, due to the restriction to buffer size, our communication objective is to minimizing the time cost for communication among the nodes.\par
	So numerate $U_p=\{(x_{pi},y_{pi},z_{pi})\}$ by taking the operation map (2.9) mentioned in subsection 2.1, which is source from $A_{pk}\times B_{pk}$ at each communication step $k$, where it should satisfy that:\par
\begin{equation}
|C_{pAk}|+|C_{pBk}|\le \text{Buffer}_p
\end{equation}
\par
	So that we can see, to parallelize the MM by adding the communication part in 3D-hypercube, the form of the operation maps can be totally conserved, and so construct a union of a series of operation maps in the same forms:
\begin{equation}
\begin{array}{ll}
&(f_{p1})_{\mathbb{R},\times}:C_{pA}\times C_{pB}\to C_{p,mult3D}\\
\Longrightarrow &
C_{p,mult3D}=\bigcup_{k=1}^{\text{total steps }K}C_{p,mult3D,k}=\bigcup_{k=1}^{K}C_{pAk}\times C_{pBk}
\end{array}
\end{equation}
Now to minimize the communication cost on each node with fixed limit of buffer size, we want to prove that:\par
\begin{theorem}
To numerate $U_p=\{(x_{pi},y_{pi},z_{pi})\}$, for $\{C_{pAk}\}$ and $\{{C_pBk}\}$ satisfying (3.5), there can exist $\{{C_{pAk}}\}$ and $\{C_{pBk}\}$ so that:
\begin{itemize}
\item For $\bigtriangleup k>1,d\in C_{pAk}\cap C_{pA,(k+\bigtriangleup k)}$ only if $d\in C_{pAk}\cap C_{pA,(k+\bigtriangleup k-1)}$.
\item For $\bigtriangleup k>1,d\in C_{pBk}\cap C_{pB,(k+\bigtriangleup k)}$ only if $d\in C_{pBk}\cap C_{pB,(k+\bigtriangleup k-1)}$.
\end{itemize}
if given that $\text{Buffer}_p\ge \min\{m_p,n_p\}$.
\end{theorem}
\par
	This theorem states the availability of Rule 3.3, so that no data (entry or block) would be sent to any one node redundantly. And the given condition can propose a general strategy for send-receive and so prove the existence.\par
	This strategy with the condition $\text{Buffer}_p\ge \min\{m_p,n_p\}$ can be stated as the following:

%% --Algorithm 3.1--%%
\begin{algorithm}[h] 
\caption{(3D-Hyperblock Algorithm with Communication)}
\begin{adjustwidth}{0cm}{0cm} 
\textbf{Input:} $A\in \mathbb{R}^{m\times l},B\in \mathbb{R}^{l\times n}$
			$,m_p,l_p,n_p,\text{Buffer}_p\in N^+,(m_p\le n_p)$\\
\textbf{Output:} $C\in \mathbb{R}^{m\times n}$\\
\textbf{if} at rank $p$\textbf{ do}
\end{adjustwidth}
	\begin{adjustwidth}{1cm}{0cm}
	$\text{Choose }U_p$
	\begin{minipage}[t]{\linewidth}
	$=\{(i_1,j_1,k_1),(i_2,j_2,k_2),...,(i_K,j_K,k_K)\} $\\
	$\subset(\mathbb{Z}_{m_p}\times\mathbb{Z}_{l_p}\times\mathbb{Z}_{n_p})\bigoplus(i_{row},j_{row},k_{row})$
	\end{minipage}

	Let $C_{pA}=(Z_(m_p)\times Z_(l_p))\bigoplus(i_low,j_low)$ and
	\par $C_{pB}=(Z_(l_p)\times Z_(n_p))\bigoplus(j_{low},k_{low})$.\\
	Sort $U_p, C_{pA}$ and $C_{pB}$ increasingly with $j$.

	Let $\text{StepSize}_A$ = $\lfloor\text{Buffer}_p/m_p\rfloor$,and initialize the buffer as:
	\begin{equation*}
		C_{pA,0}=C_{pA}[0:\text{StepSize}_A],C_{pB,0}=C_{pB}[0:\text{Buffer}_p-\text{StepSize}_A]
	\end{equation*}
	
	Set numbers of steps associated to two sets as $k_A=k_B=0$.\\
	\par
	\textbf{for }$i,j,k$ in $U_p$,
		\begin{adjustwidth}{1cm}{0cm}
		\textbf{if }$(i,j)\in C_{pA,k_A}$ and $(j,k)\in C_{pB,k_B}$ \textbf{do}
			\begin{adjustwidth}{1cm}{0cm}
			$C_{ik}=C_{ik}+A_{ij}\cdot B_{jk}$\\
			ready to send $(i,j)\in C_{pA,k_A}$ and $(j,k)\in C_{pB,k_B}$
			\end{adjustwidth}

		\textbf{else if }$(i,j)\notin C_{pA,k_A}$\textbf{do}
			\begin{adjustwidth}{1cm}{0cm}
			require $(i,j)$ from another node,\\
			pop the front of $C_{pA,k_A}$,\\
			push $(i,j)$ into the back of $C_{pA,k_A}$,\\
			$k_A=k_A+1$.
			\end{adjustwidth}

		\textbf{else if }$(j,k)\notin C_{pB,k_B}$\textbf{do}
			\begin{adjustwidth}{1cm}{0cm}
			require $(j,k)$ from another node,\\
			pop the front of $C_{pB,k_B}$,\\
			push $(j,k)$ into the back of $C_{pB,k_B}$,\\
			$k_B=k_B+1$.
			\end{adjustwidth}
		\end{adjustwidth}
	Gather the results onto the root node.
	\end{adjustwidth}

\begin{adjustwidth}{0cm}{1cm} 
\textbf{Return } $C$\\
\end{adjustwidth}
\end{algorithm}
~\\
\par
This algorithm designs a communication tree for each node, with which we can construct an algorithm with only the following two given conditions:
\begin{itemize}
\item $m_p,l_p,n_p$,i.e. a cube assigned to each node $p$;
\item $\text{Buffer}_p$.
\end{itemize}
\par
And the algorithm designed based on algorithm 3.1 is called the \textbf{Buffer Adaptive Matrix Multiplication Algorithm (BAMMA)}, which can fit to pair of matrices with random dimensions and make the best usage of buffer onside each node.\par
Next we will evaluate the performance of BAMMA, based on the variables raised in section 3.1, and show the general prediction of parallel efficiency of the algorithms for PMM in 3D-Hypercube family.\par

%% ------------ Subsection 3.3 ------------ %%
\subsection{Evaluation of Performance}
To evaluate the performance, or say the total cost of time of a PMM algorithm, we need to evaluate both the computational cost and the communication cost, i.e.:
\begin{equation}
T=\max \limits_{p}\{T_{p,comm}+T_{p,comp}+T_{p,idle}\} 
\end{equation}
where the $T_{p,idle}$ is the waiting time on node p.\par

	If we assigned similar cubes to $p$ nodes with similar limits of buffer sizes, then we may assume that there is a way to minimize $T_{p,idle}$ by building static routines for the whole communication forest. So that we may remove the maximum in (3.7) to estimate the total cost:
\begin{equation}
T_{total,P}\approx T_{p,comp}+T_{p,idle}
\end{equation}

	And considering the practical cases in computational clusters, we assume that the average time cost on one pair of send-receive process on one floating number (notated as $\hat{t}_{send-recv}$), and that of one arithmetical operation, (notated as $\hat{t}_{op}$), will satisfy that:
\begin{equation}
\gamma := \frac{\hat{t}_{send-recv}}{\hat{t}_{op}}
\end{equation}
and for practical cases, we assume $\tau$ is a constant value associated with particular machine, cluster topology, to be used to estimate the real fraction while running the program.\par
Now firstly, for the computational cost $T_{p,comp}$, by the operation map defined as (2.10), it sources from two parts: scalar (or block) multiplication and addition, so that we have:
\begin{equation}
T_{p,comp}=T_{p,mult}+T_{p,add}=(2\cdot m_pl_pn_p-m_pn_p)\cdot\hat{t}_{op}\approx 2m_pl_pn_p\cdot\hat{t}_{op}
\end{equation}
\par
	And for those similar $p$ cubes:
\begin{equation}
m_pl_pn_p\approx \frac{mln}{P}
\end{equation}
where P is the total number of nodes. So in total:
\begin{equation}
T_{comp}=T_{p,comp}\approx \frac{2mln}{P}\cdot\hat{t}_{op}
\end{equation}
\par
Now consider the next part of communication, it consists of two parts: send-receive in order to complete all the computations onside each cube, and the reduction with summation:
\begin{equation}
T_{comm}=T_{send-recv}+T_{reduce}
\end{equation}
\par
	For reduction, since the total cost is just the total number of entries (or blocks) need to be sent to the root node, along the z-axis, or say it is just the total projection area onto the xy-plane from the $P$ nodes. And since for each step, it involves one addition and one send/receive action, thus:
\begin{equation}
T_{send-recv}\ge C\cdot\max \limits_{p}\{m_pl_p+n_pl_p-\text{Buffer}_p\}\cdot\hat{t}_{send-recv}
\end{equation}
where generally $C$ is a constant, usually equals or larger than 2 for machines assigned with similar conditions, since one node can only do send or receive action at one single step.\par
	So in summary, we add all of the costs of parts respectively together, and it deduces the prediction formula of the total cost of time:
\begin{equation}
T_{total,P}\ge \frac{2mln}{P}\cdot\hat{t}_{op}+(\hat{t}_{send-recv}+\hat{t}_{op})\cdot
			\sum_{p=1}^{P}m_pn_p+\hat{t}_{send-recv}\cdot 2
			\cdot \max \limits_{p}\{m_pl_p+n_pl_p-\text{Buffer}_p\}
\end{equation}
\par
For the convenience to construct the space to include all the algorithms in 3D-Hypercube family, we define two new variables: \textbf{Z-variance} and \textbf{Maximum Saturabiliity}, denoted by $A_z$ and $\bar{S}$ respectively, as the following:
\begin{equation}
A_z:=\frac{\sum_{p=1}^{P}m_pn_p}{mn},
\bar{S}:=1-\max \limits{p}\{\frac{m_pl_p+n_pl_p-\text{Buffer}_p}{l(m+n)}\}
\end{equation}
here $A_z\ge 1$ and $S\in(0,1]$.\par

And by parallelizing the reducing process, the whole process to “compress” the cubes can be optimized into a flipped binary tree, so that it would take much shorter time than that in (3.13):
\begin{equation}
A_z:=\frac{\sum_{p=1}^{P}m_pn_p}{mn},
T_{reduce}=(\hat{t}_{send-recv}+\hat{t}_{op})\cdot \log_{2}A_z\cdot mn\cdot\frac{A_z}{P}
\end{equation}
with considering that the columns involving different cores can reduce parallely.\par

	And based on the assumptions of similar cubes we made above, we may deduce that:
\begin{equation}
\bar{S}:=1-\frac{\bar{m}_p\bar{l}_p+\bar{n}_p\bar{l}_p-\bar{\text{Buffer}}_p}{l(m+n)}
\end{equation}
where $\bar{m}_p,\bar{l}_p, \bar{n}_p$ and $\bar{\text{Buffer}}_p$ are set to be the similar for every node.\par

	Then consider the total cost of the MM on a single node is always:
\begin{equation}
T_{single}=(2mln-mn)\cdot\hat{t}_{op}
\end{equation}
\par
So onside the $A_z-\bar{S}$ plane, we can now sketch the parallel efficiency:
\begin{equation}
\renewcommand\arraystretch{1.75}
\begin{array}{rl}
\tau(m,l,n,P)\le&\frac{1}{P}\cdot\frac{(2mln-mn)\cdot\hat{t}_{op}}
				 {\frac{2mln-mn}{P}\cdot\hat{t}_{op}+(\hat{t}_{send-recv}+
				  \hat{t}_{op})\cdot \log_{2}A_z\cdot mn\cdot\frac{A_z}{P}+
				  \hat{t}_{send-recv}\cdot 2\cdot l(m+n)(1-\hat{S})}\\
\approx&\frac{mln}{mln+(\frac{1+\gamma}{2}\cdot \log_{2}A_z\cdot mn\cdot\frac{A_z}+
				   P\cdot \gamma\cdot l(m+n)(1-\bar{S}))}
\end{array}
\end{equation}

because in most of the cases, $l\gg 1$ and simplify (3.18) we can finally get:
\begin{equation}
\tau(m,l,n,P)\le\frac{1}{1+\frac{1+\gamma}{2l}\cdot A_z\cdot \log_{2}A_z+P\cdot \gamma\cdot(\frac{1}{m}+\frac{1}{n})(1-\hat{S})}
\end{equation}

\par
Here to normalize the space, we set $\hat{A}_z=1-\frac{\log_{2}A_z}{\log_{2}P}=1-\log_{P}A_z\in(0,1]$, since each node can at most own projection area mn onto xy-plane, and for any one column along the z-axis, there are at most P segments belong to different nodes. So that:
\begin{equation}
\tau(m,l,n,P)\le\frac{1}{1+\frac{1+\gamma}{2l}\cdot \log_{2}P\cdot P^{1-\hat{A}_z}\cdot(1-\hat{A}_z)
			+ P\cdot \gamma\cdot(\frac{1}{m}+\frac{1}{n})(1-\hat{S})}
\end{equation}
\par
Now with formula (3.22), we can easily predict the upper bound of the parallel efficiency by given certain $m,l,n,P,\hat{m}_p,\hat{l}_p,\hat{n}_p$ and $\\hat{text{Buffer}}_p$, and we can also locate the algorithms inside the space, with the metric of parallel efficiency.\par

%% ------------ Subsection 3.4 ------------ %%
\subsection{3D-Hypercube Family}
Out of vision of the algorithm designer, it cannot be directly indicated from formula (3.22) that how to choose $\{m_p,l_p,n_p\}$ for given $m,l,n$ and $\text{Buffer}_p$, but with which we may estimate the performance of a parallel algorithm for matrices with certain dimensions, and so support us to promote the algorithm.\par
For classical method, such as Cannon’s method[2], Fox method[1], and SUMMA[3], they all focus on dividing the hypercube into cubes with special shapes following some certain rules, as shown in the following figure for example:
% Figure: Classical Methods
\begin{figure}[h]
	\includegraphics[scale=0.2]{Figures/3DAlg.jpg}
	\caption{Some Classical Methods}
\end{figure}
\par
	But due to different dimensions of matrices involved in the multiplication, one certain strategy to design the shape of cubes can comparatively result in different performances. Here we may plot three different color maps to show the distributions of 3D-Hypercube algorithms for PMM, with the metric of parallel efficiency $\tau_{no gather}(m,l,n,P)$ defined in (3.22), under 3 different sets of dimensions, namely $m,l$ and $n$:
% Figures Performances
\begin{figure}[h]
	\includegraphics[scale=0.35]{Figures/Regular.png}
	\caption{Efficiency Distribution: Square Matrices $_{(m=l=n=1024,\tau=1)}$}
\end{figure}
\begin{figure}[h]
	\includegraphics[scale=0.35]{Figures/Tall.png}
	\caption{Efficiency Distribution: Tall Matrices $_{(m=n=512,l=4096,\tau=1)}$}
\end{figure}
\begin{figure}[h]
	\includegraphics[scale=0.35]{Figures/Flat.png}
	\caption{Efficiency Distribution: Flat Matrices $_{(m=n=2048,l=256,\tau=1)}$}
\end{figure}
~\\~\\~\\~\\~\\
\par
So from the colormaps above, we may conclude some general ideas for constructing the algorithms for PMM based on 3D-Hypercube:
\begin{itemize}
\item It would be hard to increase $\hat{A}_z$ and $\hat{S}$ with limited buffer size;
\item When $l$ is larger than $m$ and $n$, it would be better to be prior to slice the whole hypercube along z-axis, which means smaller $\hat{S}$ can compromise to smaller $\hat{A}_z$;
\item When $l$ is smaller than $m$ and $n$, it would be better to let each cube cover more blocks along the z-axis;\
\end{itemize}
\par
And we have also raised some observations from the defined variables up to now:
\begin{itemize}
\item If a cube on node $P$ is more regular, i.e. $m_p,l_p$ and $n_p$ are closer to each other, then with fixed volume $m_pl_pn_p, m_pl_p+n_pl_p$ can be smaller so that it can be more possible to increase $\hat{S}$;
\item The larger the limit of buffer size $\text{Buffer}_p$ is, the more that both $\hat{A}_z$ and $\hat{S}$ can increase;
\item For matrices with larger scale, the proportion of computations would be larger, so that the distribution of such colormap would be closer to 1.
\end{itemize}
\par
Based on those strategies, though we cannot reduce the total cost of computation, we can estimate and help to search out the best 3D-Hypercube algorithm for PMM, adaptive to certain matrices dimensions, limited buffer size and some other specific conditions.\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application of 3d-hypercube algorithm}
	In this section, we designed a series of experiments to verify our estimations of parallel efficiency using formula (3.22).\par
	From formula (3.22), we want to find out the relationship between the parallel efficiency of a constructed 3D-hypercube algorithm and a few variables, namely the dimensions of matrices, buffer size, and the number of nodes.\par
	We designed the experiments consist of two parts: one for \textbf{estimating $\tau$} and the other one for \textbf{measuring time cost for particular algorithms}, all the experiments were tested in the SeaWulf Clusters%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\footnote{Seawulf Clusters: \url{https://it.stonybrook.edu/help/kb/understanding-seawulf}}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
, and run with the same one testing program, which is developed with MPI for C/C++ and is capable of parallel communication, and PMM of complexity $O(N^3)$.\par

\subsection{Estimating the Value of \textbf{$\tau$}}
	As mentioned in Subsection 3.3 and referring to definition (18), $\tau$ is presumed as a constant value, associated with particular clusters with P cores. We ran the testing program to test two jobs: one for multiplying two matrices, both of dimensions $1024\times 1024$, and the other ones for broadcasting an $1024\times 1024$ matrix from each of $P$ cores one by one, so that to measuring the average costs, on unit send-receive communication, and floating operation.\par
	For taking the average, we have run two jobs both for 10 times, in 8, 32 and 64 cores assigned by SeaWulf
respectively. And the results%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\footnote{The detailed results may be referred in Appendix A.}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 are shown as below:\par


\begin{table}[h]
\caption{Statistics of Communication and Computation Tests}
\includegraphics[scale=0.9]{figures/table_1.png}
\end{table}

So with these data, we can calculate $\gamma$ respective to each $P$ following the deduction below:

\begin{equation}
	\frac{T_{becast}}{T_{mult}} = \frac{P(P-1)\cdot 1024^{2} \cdot \hat{t}_{sendrecv}}{(C_{program} \cdot 1024^3 - 1024^2) \cdot \hat{t}_{op} } = \frac{P(P-1)}{C_{program} \cdot 1024 -1} \cdot \gamma
\end{equation}

Here $C_program$ means the account of floating operations executed per step in a practical program for MM. However, in practical cases, it is tough to measure $C_program$ stably and precisely, so compromising to experimental cases, formula (3.21) is modified as the following:

\begin{equation} 
	%need to fix how to add  mark down note
	\renewcommand\arraystretch{1.75}
	\begin{array}{rl}
	\tau(m,l,n,P) =& \frac{1}{P} \cdot \frac{mln \cdot T_{mult \_ per \_ step}}{\frac{mln}{P} \cdot T_{mult \_ per \_ step} + (\hat{t}_{send-recv} + \hat{t}_{op}) \cdot \log_{2} A_{z} \cdot mn \cdot \frac{A_{z}}{P} + \hat{t}_{sendrecv} \cdot 2 \cdot l}\\ 
	=& \frac{1}{1 + \frac{\hat{t}_{sendrecv} + \hat{t}_{op}}{T_{mult \_ per \_ step} } \cdot \frac{\log_{2} \hat{A}_{z}}{l} \cdot \hat{A}_{z} + \frac{2 \cdot \hat{t}_send-recv}{T_{mult \_ per \_ step}} \cdot P \cdot (\frac{1}{m} + \frac{1}{n})(1- \bar{S}) }
	\end{array}
\end{equation}

Here $T_{mult \_ per \_ step}$ is the time cost per step in the loop of a MM in programming.
		
		Then comparing to formula (3.22), it is reasonable to obtain the following relationship:

\begin{equation}
\gamma \approx \frac{2 \cdot \hat{t}_{sendrecv}}{T_{mult_per_step}} = 2 \cdot \frac{T_{bcast} / P(P-1) \cdot 1024^2}{T_{mult}/(1024^3)} = \frac{2048}{P(P-1)} \cdot \frac{T_{bcast}}{T_{mult}}
\end{equation}

So the estimated value of each $\gamma$ is calculated as the following:

\begin{table}[h]
\caption{$\gamma$ Values Calculated from the Experiments}
\includegraphics[scale=0.8]{figures/table_2.png}
\end{table}
\par
It has to be admitted that there values are less than the real ones due to those not considered, and the communication performance is usually more complicated in parallel program, than that analyzed in theory so that hard to obtain the correct $\gamma$.

\subsection{Experiments of Particular Algorithms}

For each cases with 8, 32 and 64 cores respectively, we designed 7, 10 and 10 different strategies to split the 3D-hypercube and allocate the buffers in different way, so that constructed various algorithms for testing the performances of each. 

To evaluate the performance, the formula for calculating the parallel efficiency is applied here:

\begin{equation}
\tau = \frac{1}{P} \cdot \frac{T_{one \_ core}}{T_{P \_ Cores}}
\end{equation}

The plans with results%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\footnote{The detailed results may be referred in Appendix B.}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 are shown as the following subsections:

\subsubsection{$P = 8$} (total 7 cases)\\
\includegraphics[scale=0.41]{figures/Experiments_P8.jpg}

\subsubsection{$P = 32$} (total 10 cases)\\
\includegraphics[scale=0.38]{figures/Experiments_P32.jpg}

\subsubsection{$P = 64$} (total 10 cases)\\
\includegraphics[scale=0.38]{figures/Experiments_P64.jpg}

Therefore, finally we can plot the experimental results inside a 3D space together with the expected surfaces sketched from formula (3.22), which is shown as the following:
\begin{figure}[h]
	\includegraphics[scale=0.7]{figures/Efficieny.png}
	\caption{Efficiency Distribution with Experiments Results}
\end{figure}


Comparing to the analytic surfaces, we found that the communication parts of the algorithms are indeed affected by more other elements not discussed in this paper, which to be searched out is part of the future work on BAMMA.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and future work}
	In this paper, we have discussed about the construction of algorithms for PMM, based on 3D-Hypercube so that with total computational complexity $O(N^3)$. We have observed that the construction of such algorithm is influenced by the number of computational nodes, the dimensions of matrices and the limit of the buffer size. Concluded from both the theoretical analysis and the experiments, it is learnt that often it is better to construct the parallel algorithms considering adaptive to certain cases, described by the three variables mentioned above, rather than primly follow the rules defined by classical methods.
	
	All of the theoretical analysis is based on the assumption that all the cubes and communication trees onside the P nodes are designed similarly. So as another part of our future work on PMM, we are going to dig out the possibility to design and evaluate the algorithm, with more flexibility on each specific node with specific condition respectively, so that it can adapt to more practical cases with the best performance. As one of the most latest coming work, we are going to optimize the algorithms on clusters with particular or even special topologies, using the ideas raised in this paper.
	
	And as mentioned in Section 3.2, another one of our research plans in the future is to optimize and finish the program of BAMMA, so that it can satisfy the requirement for parallel universal MM adaptive to specific conditions, expected with better performance comparable to the classical method. And referring to a popular branch of algorithms of PMM, the recursive algorithms, for instance CARMA[4] raised by the team led by James Demmel in 2013, we may need to furtherly research on the influence of sequential recursion, on the performance of a parallel algorithm of PMM.

	Furtherly expanding the algorithm into higher dimensions, series of productions, and special matrices, is also worthwhile. Obtaining a reliable method of parallel matrix multiplication may provide a way to predict and promote the speedup and efficiency of tensor production [7]. And to follow current great researches on sparse tensor space[8], 3D-hypercube algorithm may be helpful with assiting constructing a denser space for faster computitation, with avoiding the uncessary and redundant calculations.  
	
	Finally, as the final target to optimize the algorithm for general matrix multiplications, our group has proposed a formulated problem in 4D Hypercube, with the concepts of operation map mentioned in Section 2.1, to reduce the computational complexity of matrix multiplication, of which is lower than $O(N^3)$, inspired by the ideas[5],  raised by Strassen in 1969as well as Coppersmith and Winograd[6] in 1987. And now there has been some great work on that, for instance the research on parallel Strassen on MIMD system[9]. Our next target is to find a general strategy to solve the problem so that find the limit of recursive algorithms to reduce the total complexity of matrix multiplications, and furtherly construct stable and scalable parallel algorithm for universal matrix multiplication, with lower computational complexity.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{amsplain}
\begin{thebibliography}{10}

\bibitem {F}Fox, G.C., et al., \textit{Solving problems on concurrent processors}. 
Vol. 1: General techniques and regular problems. 1988: Prentice-Hall, Inc.

\bibitem {C}Cannon, L.E., \textit{A cellular computation to implement the kalman filter algorithm}.
 1969, Montana State University Bozeman Engineering Research Labs.

\bibitem {V}Van De Geijn, R.A. and J. Watts, \textit{SUMMA: Scalable universal matrix multiplication algorithm}.
 Concurrency-Practice and Experience, 1997. 9(4): p. 255-274.

\bibitem {D}Demmel, J., et al. \textit{Communication-optimal parallel recursive rectangular matrix multiplication}.
 in Parallel \& Distributed Processing (IPDPS), 2013 IEEE 27th International Symposium on. 2013. IEEE.

\bibitem {S}Strassen, V., \textit{Gaussian elimination is not optimal}. Numerische mathematik, 1969. 13(4): p. 354-356.

\bibitem {C}Coppersmith, D. and S. Winograd. \textit{Matrix multiplication via arithmetic progressions}.
 in Proceedings of the nineteenth annual ACM symposium on Theory of computing. 1987. ACM.

\bibitem {M}M. Griebel and H. Harbrecht., \textit{On the construction of sparse tensor product spaces},
 Math. Comp. 82 (2013), 975-994.

\bibitem {F} F. Johansson. \textit{A fast algorithm for reversion of power series}. Math. Comp., 84:475–484, 2015.

\bibitem {C}Chung-Chiang Chou, Yuefan Deng, Gang Li, and Yuan Wang. 
\textit{Parallelizing Strassen's Method for Matrix Multiplication on Distributed Memory MIMD architectures}.

\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{appendix}
\section*{Appendix A: Measurement of the values of $\gamma$}
\begin{table}[h]
\caption*{Results of Experiments of Individual Computation}
\renewcommand\arraystretch{1.5}
\noindent\[
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
P&\text{Test1}&\text{Test2}&\text{Test3}&\text{Test4}&\text{Test5}&\text{Test6}&\text{Test7}&\text{Test8}&\text{Test9}&\text{Test10}\\
\hline
8&5.2368&5.2352&5.2354&5.2396&5.2383&5.2362&5.2357&5.2368&5.2355&5.2372\\
\hline
32&5.2768&5.2717&5.2759&5.2862&5.2733&5.2842&5.2752&5.2728&5.2766&5.2732\\
\hline
64&5.2709&5.2760&5.2852&5.2748&5.2739&5.2712&5.2891&5.2773&5.2777&5.2729\\
\hline
\end{array}
\]
\end{table}

\begin{table}[h]
\caption*{Results of Experiments of Broadcasting}
\renewcommand\arraystretch{1.5}
\noindent\[
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
P&\text{Test1}&\text{Test2}&\text{Test3}&\text{Test4}&\text{Test5}&\text{Test6}&\text{Test7}&\text{Test8}&\text{Test9}&\text{Test10}\\
\hline
8&5.2368&5.2352&5.2354&5.2396&5.2383&5.2362&5.2357&5.2368&5.2355&5.2372\\
\hline
32&5.2768&5.2717&5.2759&5.2862&5.2733&5.2842&5.2752&5.2728&5.2766&5.2732\\
\hline
64&5.2709&5.2760&5.2852&5.2748&5.2739&5.2712&5.2891&5.2773&5.2777&5.2729\\
\hline
\end{array}
\]
\end{table}

\newpage
\section*{Appendix B: Experiments of 3D-hypercube algorithms}
\text{$P=8, m=l=n=1024$: (in sec)}\\
\includegraphics[scale=0.5]{figures/Appendix_Alg1.jpg}\par

\text{$P=32, m=l=n=1024$: (in sec)}\\
\includegraphics[scale=0.5]{figures/Appendix_Alg2.jpg}\par

\text{$P=64, m=l=n=1024$: (in sec)}\\
\includegraphics[scale=0.5]{figures/Appendix_Alg3.jpg}\par

\end{appendix}
\newpage
\end{document}

%------------------------------------------------------------------------------
% END
%------------------------------------------------------------------------------
